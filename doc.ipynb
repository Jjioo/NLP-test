{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Charger le modèle de langue spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./td2/reaganomics.txt\", \"r\", encoding=\"latin-1\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokeniser les mots\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntechnique that involves the identification and classification of named entities\\n(such as persons, organizations, locations, medical codes, time expressions, quantities,\\nmonetary values, percentages, etc.) in text data. The goal of NER is to extract structured\\ninformation from unstructured text.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Éliminer les stop words\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "\"\"\"\n",
    "technique that involves the identification and classification of named entities\n",
    "(such as persons, organizations, locations, medical codes, time expressions, quantities,\n",
    "monetary values, percentages, etc.) in text data. The goal of NER is to extract structured\n",
    "information from unstructured text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Appliquer le Matcher sur un paragraphe\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"supply\"}, {\"LOWER\": \"side\"}]\n",
    "matcher.add(\"SupplySide\", [pattern])\n",
    "\n",
    "paragraph = \"Reaganomics was a supply-side economic policy.\"\n",
    "paragraph_doc = nlp(paragraph)\n",
    "\n",
    "matches = matcher(paragraph_doc)\n",
    "matched_phrases = [paragraph_doc[start:end].text for _, start, end in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les résultats\n",
    "#print(\"1. Texte importé:\")\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Premiers 10 mots tokenisés avec index:\n",
      "1. REAGANOMICS\n",
      "2. \n",
      "\n",
      "3. https://en.wikipedia.org/wiki/Reaganomics\n",
      "4. \n",
      "\n",
      "\n",
      "5. Reaganomics\n",
      "6. (\n",
      "7. a\n",
      "8. portmanteau\n",
      "9. of\n",
      "10. [\n"
     ]
    }
   ],
   "source": [
    "# 2. Tokeniser les mots\n",
    "doc_tokens = [(i, token.text) for i, token in enumerate(doc)]\n",
    "formatted_tokens = '\\n'.join([f\"{index + 1}. {token}\" for index, token in doc_tokens[:10]])\n",
    "\n",
    "print(\"\\n2. Premiers 10 mots tokenisés avec index:\")\n",
    "print(formatted_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Mots sans stop words avec index:\n",
      "1. REAGANOMICS\n",
      "2. \n",
      "\n",
      "3. https://en.wikipedia.org/wiki/Reaganomics\n",
      "4. \n",
      "\n",
      "\n",
      "5. Reaganomics\n",
      "6. (\n",
      "7. portmanteau\n",
      "8. [\n",
      "9. Ronald\n",
      "10. ]\n"
     ]
    }
   ],
   "source": [
    "# 3. Mots sans stop words\n",
    "filtered_tokens_with_index = [(i, token) for i, token in enumerate(filtered_tokens, start=1)]\n",
    "formatted_filtered_tokens = '\\n'.join([f\"{index}. {token}\" for index, token in filtered_tokens_with_index[:10]])\n",
    "\n",
    "print(\"\\n3. Mots sans stop words avec index:\")\n",
    "print(formatted_filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Entités nommées (NER) avec index:\n",
      "1. Texte: REAGANOMICS, Label: ORG\n",
      "2. Texte: Ronald] Reagan, Label: PERSON\n",
      "3. Texte: Paul, Label: PERSON\n",
      "4. Texte: U.S., Label: GPE\n",
      "5. Texte: Ronald Reagan, Label: PERSON\n",
      "6. Texte: the 1980s, Label: DATE\n",
      "7. Texte: four, Label: CARDINAL\n",
      "8. Texte: Reagan, Label: PERSON\n",
      "9. Texte: Reaganomics, Label: NORP\n",
      "10. Texte: the decades, Label: DATE\n"
     ]
    }
   ],
   "source": [
    "# 4. Entités nommées (NER)\n",
    "ner_entities_with_index = [(i, ent.text, ent.label_) for i, ent in enumerate(doc.ents, start=1)]\n",
    "formatted_ner_entities = '\\n'.join([f\"{index}. Texte: {text}, Label: {label}\" for index, text, label in ner_entities_with_index[:10]])\n",
    "\n",
    "# Print the named entities with indices\n",
    "print(\"\\n4. Entités nommées (NER) avec index:\")\n",
    "print(formatted_ner_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Phrases correspondant au pattern 'Supply Side':\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5. Phrases correspondant au pattern 'Supply Side':\")\n",
    "print(matched_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**---------------------------------------------------------**\\\n",
    "**---------------------** **TD3** **------------------------------**\\\n",
    "**---------------------------------------------------------**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text from \"TextGPT.txt\"\n",
    "with open(\"./td3/TextGPT.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1:\n",
      "Occurrences of 'gpt': ['GPT', 'GPT']\n"
     ]
    }
   ],
   "source": [
    "# Question 1: Find all occurrences of the word \"gpt\" using the re library\n",
    "gpt_occurrences = re.findall(r'\\bgpt\\b', text, flags=re.IGNORECASE)\n",
    "print(\"\\nQuestion 1:\")\n",
    "print(\"Occurrences of 'gpt':\", gpt_occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 2:\n",
      "Number of sentences in each paragraph: [9, 16, 14, 8, 34, 36]\n"
     ]
    }
   ],
   "source": [
    "# Question 2: Split the text into paragraphs and count the number of sentences in each paragraph\n",
    "paragraphs = re.split(r'\\n\\n', text)\n",
    "num_sentences_per_paragraph = [len(list(nlp(paragraph).sents)) for paragraph in paragraphs]\n",
    "print(\"\\nQuestion 2:\")\n",
    "print(\"Number of sentences in each paragraph:\", num_sentences_per_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 3:\n",
      "Number of words in each paragraph: [85, 150, 170, 112, 260, 326]\n"
     ]
    }
   ],
   "source": [
    "# Question 3: Tokenize the text without using split and count the number of words in each paragraph\n",
    "paragraph_tokens = [list(nlp(paragraph)) for paragraph in paragraphs]\n",
    "num_words_per_paragraph = [len(tokens) for tokens in paragraph_tokens]\n",
    "print(\"\\nQuestion 3:\")\n",
    "print(\"Number of words in each paragraph:\", num_words_per_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 4:\n",
      "Paragraphs without stop words: ['ChatGPT', ',', 'which', 'stands', 'for', 'Chat', 'Generative', 'Pre', '-', 'trained', 'Transformer', ',', 'is', 'a', 'chatbot', 'developed', 'by', 'OpenAI', '.', 'ChatGPT', 'is', 'built', 'top', 'of', \"OpenAI'\", 'GPT-3.5', 'family', 'of', 'large', 'language', 'models', ',', 'and', 'is', 'fine', '-', 'tuned', 'with', 'both', 'supervised', 'and', 'reinforcement', 'learning', 'techniques', '.', 'ChatGPT', 'was', 'launched', 'a', 'prototype', 'in', 'November', '2022', ',', 'and', 'quickly', 'garnered', 'attention', 'for', 'its', 'detailed', 'responses', 'and', 'articulate', 'answers', 'across', 'many', 'domains', 'of', 'knowledge', '.', 'Its', 'uneven', 'factual', 'accuracy', 'was', 'identified', 'a', 'significant', 'drawback.[1', ']']\n",
      "Paragraphs without stop words: ['ChatGPT', 'was', 'fine', '-', 'tuned', 'top', 'of', 'GPT-3.5', 'using', 'supervised', 'learning', 'well', 'reinforcement', 'learning.[2', ']', 'Both', 'approaches', 'used', 'human', 'trainers', 'to', 'improve', 'the', \"model'\", 'performance', '.', 'In', 'the', 'case', 'of', 'supervised', 'learning', ',', 'the', 'model', 'was', 'provided', 'with', 'conversations', 'in', 'which', 'the', 'trainers', 'played', 'both', 'sides', ':', 'the', 'user', 'and', 'the', 'assistant', '.', 'In', 'the', 'reinforcement', 'step', ',', 'human', 'trainers', 'first', 'ranked', 'responses', 'that', 'the', 'model', 'had', 'created', 'in', 'a', 'previous', 'conversation', '.', 'These', 'rankings', 'were', 'used', 'to', 'create', \"'\", 'reward', 'models', \"'\", 'that', 'the', 'model', 'was', 'further', 'fine', '-', 'tuned', 'using', 'several', 'iterations', 'of', 'Proximal', 'Policy', 'Optimization', '(', 'PPO).[3][4', ']', 'Proximal', 'Policy', 'Optimization', 'algorithms', 'present', 'a', 'cost', '-', 'effective', 'benefit', 'to', 'trust', 'region', 'policy', 'optimization', 'algorithms', ';', 'they', 'negate', 'many', 'of', 'the', 'computationally', 'expensive', 'operations', 'with', 'faster', 'performance.[5][6', ']', 'The', 'models', 'were', 'trained', 'in', 'collaboration', 'with', 'Microsoft', 'their', 'Azure', 'supercomputing', 'infrastructure', '.']\n",
      "Paragraphs without stop words: ['In', 'comparison', 'to', 'its', 'predecessor', ',', 'InstructGPT', ',', 'ChatGPT', 'attempts', 'to', 'reduce', 'harmful', 'and', 'deceitful', 'responses', ';', 'in', 'one', 'example', ',', 'while', 'InstructGPT', 'accepts', 'the', 'prompt', '\"', 'Tell', 'about', 'when', 'Christopher', 'Columbus', 'came', 'to', 'the', 'US', 'in', '2015', '\"', 'truthful', ',', 'ChatGPT', 'uses', 'information', 'about', 'Columbus', \"'\", 'voyages', 'and', 'information', 'about', 'the', 'modern', 'world', '–', 'including', 'perceptions', 'of', 'Columbus', 'to', 'construct', 'an', 'answer', 'that', 'assumes', 'what', 'would', 'happen', 'if', 'Columbus', 'came', 'to', 'the', 'U.S.', 'in', '2015.[3', ']', \"ChatGPT'\", 'training', 'data', 'includes', 'man', 'pages', 'and', 'information', 'about', 'Internet', 'phenomena', 'and', 'programming', 'languages', ',', 'such', 'bulletin', 'board', 'systems', 'and', 'the', 'Python', 'programming', 'language.[7]Unlike', 'most', 'chatbots', ',', 'ChatGPT', 'is', 'stateful', ',', 'remembering', 'previous', 'prompts', 'given', 'to', 'it', 'in', 'the', 'same', 'conversation', ',', 'which', 'some', 'journalists', 'have', 'suggested', 'will', 'allow', 'for', 'ChatGPT', 'to', 'be', 'used', 'a', 'personalized', 'therapist.[8', ']', 'To', 'prevent', 'offensive', 'outputs', 'from', 'being', 'presented', 'to', 'and', 'produced', 'from', 'ChatGPT', ',', 'queries', 'are', 'filtered', 'through', 'a', 'moderation', 'API', ',', 'and', 'potentially', 'racist', 'or', 'sexist', 'prompts', 'are', 'dismissed.[3][8', ']']\n",
      "Paragraphs without stop words: ['ChatGPT', 'suffers', 'from', 'multiple', 'limitations', '.', 'The', 'reward', 'model', 'of', 'ChatGPT', ',', 'designed', 'around', 'human', 'oversight', ',', 'can', 'be', 'over', '-', 'optimized', 'and', 'thus', 'hinder', 'performance', ',', 'otherwise', 'known', \"Goodhart'\", 'law.[9', ']', 'Furthermore', ',', 'ChatGPT', 'has', 'limited', 'knowledge', 'of', 'events', 'that', 'occurred', 'after', '2021', 'and', 'is', 'unable', 'to', 'provide', 'information', 'some', 'celebrities.[failed', 'verification', ']', 'In', 'training', ',', 'reviewers', 'preferred', 'longer', 'answers', ',', 'irrespective', 'of', 'actual', 'comprehension', 'or', 'factual', 'content.[3', ']', 'Training', 'data', 'may', 'also', 'suffer', 'from', 'algorithmic', 'bias', ';', 'prompts', 'including', 'vague', 'descriptors', 'of', 'people', ',', 'such', 'a', 'CEO', ',', 'could', 'generate', 'a', 'response', 'that', 'assumes', 'such', 'a', 'person', ',', 'for', 'instance', ',', 'is', 'a', 'white', 'male.[10', ']']\n",
      "Paragraphs without stop words: ['ChatGPT', 'was', 'met', 'in', 'December', '2022', 'with', 'generally', 'positive', 'reviews', ';', 'The', 'New', 'York', 'Times', 'labeled', 'it', '\"', 'the', 'best', 'artificial', 'intelligence', 'chatbot', 'ever', 'released', 'to', 'the', 'general', 'public\".[13', ']', 'Samantha', 'Lock', 'of', 'The', 'Guardian', 'noted', 'that', 'it', 'was', 'able', 'to', 'generate', '\"', 'impressively', 'detailed', '\"', 'and', '\"', 'human', '-', 'like', '\"', 'text.[14', ']', 'Technology', 'writer', 'Dan', 'Gillmor', 'used', 'ChatGPT', 'a', 'student', 'assignment', ',', 'and', 'found', 'its', 'generated', 'text', 'was', 'with', 'what', 'a', 'good', 'student', 'would', 'deliver', 'and', 'opined', 'that', '\"', 'academia', 'has', 'some', 'very', 'serious', 'issues', 'to', 'confront\".[15', ']', 'Alex', 'Kantrowitz', 'of', 'Slate', 'lauded', \"ChatGPT'\", 'pushback', 'to', 'questions', 'related', 'to', 'Nazi', 'Germany', ',', 'including', 'the', 'claim', 'that', 'Adolf', 'Hitler', 'built', 'highways', 'in', 'Germany', ',', 'which', 'was', 'met', 'with', 'information', 'regarding', 'Nazi', \"Germany'\", 'use', 'of', 'forced', 'labor.[16', ']', 'In', 'The', \"Atlantic'\", '\"', 'Breakthroughs', 'of', 'the', 'Year', '\"', 'for', '2022', ',', 'Derek', 'Thompson', 'included', 'ChatGPT', 'part', 'of', '\"', 'the', 'generative', '-', 'eruption', '\"', 'that', '\"', 'may', 'change', 'our', 'mind', 'about', 'how', 'we', 'work', ',', 'how', 'we', 'think', ',', 'and', 'what', 'human', 'creativity', 'really', 'is\".[17', ']', 'Kelsey', 'Piper', 'of', 'Vox', 'wrote', 'that', '\"', 'ChatGPT', 'is', 'the', 'general', \"public'\", 'first', 'hands', '-on', 'introduction', 'to', 'how', 'powerful', 'modern', 'has', 'gotten', ',', 'and', 'a', 'result', ',', 'many', 'of', 'us', 'are', '(', 'stunned', ')', '\"', 'and', 'that', '\"', 'ChatGPT', 'is', 'smart', 'enough', 'to', 'be', 'useful', 'despite', 'its', 'flaws', '\"', '.', 'In', 'a', 'tweet', ',', 'tech', 'mogul', 'Elon', 'Musk', 'wrote', 'that', '\"', 'ChatGPT', 'is', 'scary', 'good', '.', 'We', 'are', 'not', 'far', 'from', 'dangerously', 'strong', 'AI\".[18', ']']\n",
      "Paragraphs without stop words: ['In', 'a', 'December', '2022', 'opinion', 'piece', ',', 'economist', 'Paul', 'Krugman', 'wrote', 'that', 'ChatGPT', 'would', 'affect', 'the', 'demand', 'of', 'knowledge', 'workers.[19', ']', 'The', \"Verge'\", 'James', 'Vincent', 'saw', 'the', 'viral', 'success', 'of', 'ChatGPT', 'evidence', 'that', 'artificial', 'intelligence', 'had', 'gone', 'mainstream.[4', ']', 'Journalists', 'have', 'commented', \"ChatGPT'\", 'tendency', 'to', 'hallucinate', '(', 'confidently', 'give', 'false', 'answers', 'that', 'seem', 'unjustified', 'by', 'its', 'training', 'data).[20', ']', 'Mike', 'Pearl', 'of', 'Mashable', 'tested', 'ChatGPT', 'with', 'multiple', 'questions', '.', 'In', 'one', 'example', ',', 'he', 'asked', 'the', 'model', 'for', '\"', 'the', 'largest', 'country', 'in', 'Central', 'America', 'that', \"isn'\", 'Mexico', '\"', '.', 'ChatGPT', 'responded', 'with', 'Guatemala', ',', 'when', 'the', 'answer', 'is', 'instead', 'Nicaragua.[21', ']', 'When', 'CNBC', 'asked', 'ChatGPT', 'for', 'the', 'lyrics', 'to', '\"', 'The', 'Ballad', 'of', 'Dwight', 'Fry', '\"', ',', 'ChatGPT', 'supplied', 'invented', 'lyrics', 'rather', 'than', 'the', 'actual', 'lyrics.[12', ']', 'In', 'contrast', ',', 'researchers', 'cited', 'by', 'The', 'Verge', 'compared', 'ChatGPT', 'to', 'a', '\"', 'stochastic', 'parrot\",[22', ']', 'did', 'Professor', 'Anton', 'Van', 'Den', 'Hengel', 'of', 'the', 'Australian', 'Institute', 'for', 'Machine', 'Learning.[23', ']', 'In', 'December', '2022', ',', 'the', 'question', 'and', 'answer', 'website', 'Stack', 'Overflow', 'banned', 'the', 'use', 'of', 'ChatGPT', 'for', 'generating', 'answers', 'to', 'questions', ',', 'citing', 'the', 'factually', 'ambiguous', 'nature', 'of', \"ChatGPT'\", 'responses.[1', ']', 'Economist', 'Tyler', 'Cowen', 'expressed', 'concerns', 'regarding', 'its', 'effects', 'democracy', ',', 'citing', 'the', 'ability', 'of', 'one', 'to', 'write', 'automated', 'comments', 'in', 'an', 'effort', 'to', 'affect', 'the', 'decision', 'process', 'of', 'new', 'regulations.[24', ']', 'The', 'Guardian', 'questioned', 'whether', 'any', 'content', 'found', 'the', 'Internet', 'after', \"ChatGPT'\", 'release', '\"', 'can', 'be', 'truly', 'trusted', '\"', 'and', 'called', 'for', 'government', 'regulation.[25', ']', 'Ax', 'Sharma', 'of', 'Bleeping', 'Computer', 'noted', 'that', 'ChatGPT', 'was', 'capable', 'of', 'writing', 'malware', 'and', 'phishing', 'emails.[26', ']', 'The', 'CEO', 'of', 'ChatGPT', 'creator', 'OpenAI', ',', 'Sam', 'Altman', ',', 'wrote', 'that', 'advancing', 'software', 'could', 'pose', '\"', '(', 'for', 'example', ')', 'a', 'huge', 'cybersecurity', 'risk', '\"', 'and', 'also', 'continued', 'to', 'predict', '\"', 'we', 'could', 'get', 'to', 'real', 'AGI', 'in', 'the', 'next', 'decade', ',', 'so', 'we', 'have', 'to', 'take', 'the', 'risk', 'of', 'that', 'extremely', 'seriously\".[11', ']']\n"
     ]
    }
   ],
   "source": [
    "# Question 4: Eliminate stop words\n",
    "stop_words = set(stopwords.words(\"french\"))\n",
    "filtered_paragraphs = [\n",
    "    [token.text for token in tokens if token.text.lower() not in stop_words]\n",
    "    for tokens in paragraph_tokens\n",
    "]\n",
    "print(\"\\nQuestion 4:\")\n",
    "for _ in range(len(filtered_paragraphs)):\n",
    "    print(\"Paragraphs without stop words:\", filtered_paragraphs[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 5:\n",
      "POS tags for the first paragraph: ['VERB', 'PUNCT', 'NOUN', 'ADJ', 'ADP', 'NOUN', 'ADJ', 'PROPN', 'NOUN', 'NOUN', 'PROPN', 'PUNCT', 'ADP', 'AUX', 'VERB', 'PROPN', 'ADP', 'ADV', 'PUNCT', 'VERB', 'ADP', 'NOUN', 'PRON', 'AUX', 'PRON', 'VERB', 'AUX', 'PROPN', 'PROPN', 'ADP', 'ADJ', 'NOUN', 'ADJ', 'PUNCT', 'X', 'VERB', 'NOUN', 'PROPN', 'ADJ', 'PROPN', 'PROPN', 'X', 'X', 'VERB', 'ADJ', 'ADJ', 'PUNCT', 'NOUN', 'X', 'PROPN', 'PRON', 'AUX', 'VERB', 'X', 'PROPN', 'NUM', 'PUNCT', 'X', 'X', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADJ', 'X', 'ADJ', 'ADJ', 'ADJ', 'NOUN', 'ADJ', 'ADP', 'NOUN', 'PUNCT', 'ADP', 'PROPN', 'NOUN', 'ADJ', 'AUX', 'ADJ', 'PRON', 'VERB', 'VERB', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "# Question 5: Apply POS tagging to the first paragraph\n",
    "pos_tags_first_paragraph = [token.pos_ for token in paragraph_tokens[0]]\n",
    "print(\"\\nQuestion 5:\")\n",
    "print(\"POS tags for the first paragraph:\", pos_tags_first_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 6:\n",
      "Bag of Words matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 2 1 1]\n",
      " [0 1 1 ... 2 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Question 6: Apply Bag of Words (BOW) to each paragraph\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform([' '.join(tokens) for tokens in filtered_paragraphs])\n",
    "print(\"\\nQuestion 6:\")\n",
    "print(\"Bag of Words matrix:\")\n",
    "print(bow_matrix.toarray())\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Bag of Words (BOW) to each paragraph involves representing the text data in a numerical format that machine learning algorithms can understand. BOW is a common technique used in natural language processing and text analysis.\n",
    "\n",
    "Here's a step-by-step explanation of how BOW works:\n",
    "\n",
    "1. **Tokenization:**\n",
    "   Break each paragraph into individual words or tokens. This process involves separating the text into meaningful units, such as words.\n",
    "\n",
    "2. **Vocabulary Creation:**\n",
    "   Create a vocabulary, which is a unique set of all the words in the entire collection of paragraphs. Each word in the vocabulary is assigned a unique index.\n",
    "\n",
    "3. **Vectorization:**\n",
    "   For each paragraph, create a vector (array) of numerical values based on the words in the paragraph and their frequency in the vocabulary. The length of the vector is equal to the size of the vocabulary.\n",
    "\n",
    "   - If a word from the vocabulary is present in the paragraph, the corresponding element in the vector is set to the frequency of that word in the paragraph.\n",
    "   - If a word is not present, the corresponding element is set to zero.\n",
    "\n",
    "   This process results in a numerical representation of each paragraph in the form of a vector. Each element in the vector represents the frequency of a particular word from the vocabulary in the corresponding paragraph.\n",
    "\n",
    "Here's a simplified example:\n",
    "\n",
    "Consider the vocabulary: [\"apple\", \"orange\", \"banana\"]\n",
    "\n",
    "Paragraph 1: \"I like apples and bananas.\"\n",
    "Vector for Paragraph 1: [1, 0, 1]\n",
    "\n",
    "Paragraph 2: \"I like oranges.\"\n",
    "Vector for Paragraph 2: [0, 1, 0]\n",
    "\n",
    "This way, you've converted text data into a format that machine learning algorithms can use for tasks like classification or clustering. The BOW representation doesn't capture word order or semantics but focuses on the occurrence of words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 7:\n",
      "TF-IDF matrix:\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.11872813 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.10092691 0.06153972 0.06153972]\n",
      " [0.         0.04684914 0.04684914 ... 0.07683394 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Question 7: Calculate TF-IDF for each word\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(tokens) for tokens in filtered_paragraphs])\n",
    "print(\"\\nQuestion 7:\")\n",
    "print(\"TF-IDF matrix:\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8: Load GloVe embeddings into a dictionary\n",
    "glove_embeddings = {}\n",
    "with open(\"D:/NLP/td3/glove.6B.50d (1).txt\", \"r\", encoding=\"utf-8\") as glove_file:\n",
    "    for line in glove_file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 9:\n",
      "Cosine similarity between first 10 and next 10 words based on TF-IDF-weighted GloVe vectors: 0.8737339172893369\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get the first 10 and next 10 words of the first paragraph\n",
    "first_10_words = filtered_paragraphs[0][:10]\n",
    "next_10_words = filtered_paragraphs[0][10:20]\n",
    "\n",
    "# Get the indices of the words in the TF-IDF matrix\n",
    "indices_first_10 = [tfidf_vectorizer.vocabulary_[word] for word in first_10_words if word in tfidf_vectorizer.vocabulary_]\n",
    "indices_next_10 = [tfidf_vectorizer.vocabulary_[word] for word in next_10_words if word in tfidf_vectorizer.vocabulary_]\n",
    "\n",
    "# Get TF-IDF scores for the first 10 and next 10 words\n",
    "tfidf_scores_first_10 = tfidf_matrix[0, indices_first_10].toarray()[0]\n",
    "tfidf_scores_next_10 = tfidf_matrix[0, indices_next_10].toarray()[0]\n",
    "\n",
    "# Get GloVe embeddings for the first 10 and next 10 words from TF-IDF matrix\n",
    "glove_embeddings_first_10 = np.array([glove_embeddings.get(word, np.zeros(50)) for word in tfidf_vectorizer.get_feature_names_out()[indices_first_10]])\n",
    "glove_embeddings_next_10 = np.array([glove_embeddings.get(word, np.zeros(50)) for word in tfidf_vectorizer.get_feature_names_out()[indices_next_10]])\n",
    "\n",
    "# Calculate TF-IDF weighted GloVe vectors for each word\n",
    "weighted_vectors_first_10 = glove_embeddings_first_10 * tfidf_scores_first_10[:, np.newaxis]\n",
    "weighted_vectors_next_10 = glove_embeddings_next_10 * tfidf_scores_next_10[:, np.newaxis]\n",
    "\n",
    "# Calculate cosine similarity between average vectors\n",
    "avg_vector_first_10 = np.mean(weighted_vectors_first_10, axis=0)\n",
    "avg_vector_next_10 = np.mean(weighted_vectors_next_10, axis=0)\n",
    "\n",
    "similarity = cosine_similarity(avg_vector_first_10.reshape(1, -1), avg_vector_next_10.reshape(1, -1))\n",
    "\n",
    "print(\"\\nQuestion 9:\")\n",
    "print(\"Cosine similarity between first 10 and next 10 words based on TF-IDF-weighted GloVe vectors:\", similarity[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXAM 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de mots: 6181\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Question 1: Trouver le nombre total de mots avec NLTK\n",
    "with open('./td2/reaganomics.txt', 'r', encoding='latin-1') as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "num_words = len(tokens)\n",
    "print(\"Nombre total de mots:\", num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entité nommée 1: REAGANOMICS\n",
      "https://en.wikipedia.org/wiki/Reaganomics\n",
      "\n",
      "Reaganomics, Type: ORG\n",
      "Entité nommée 2: Ronald] Reagan, Type: PERSON\n",
      "Entité nommée 3: Paul Harvey)[1, Type: PERSON\n",
      "Entité nommée 4: U.S., Type: GPE\n",
      "Entité nommée 5: Ronald Reagan, Type: PERSON\n",
      "Entité nommée 6: the 1980s, Type: DATE\n",
      "Entité nommée 7: four, Type: CARDINAL\n",
      "Entité nommée 8: Reagan, Type: PERSON\n",
      "Entité nommée 9: Reaganomics, Type: PERSON\n",
      "Entité nommée 10: the decades, Type: DATE\n"
     ]
    }
   ],
   "source": [
    "# Question 2: Identifier et afficher les 10 premières entités nommées avec spaCy\n",
    "doc = nlp(text)\n",
    "for i, ent in enumerate(doc.ents[:10]):\n",
    "    print(f\"Entité nommée {i+1}: {ent.text}, Type: {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Token 0: REAGANOMICS\n",
      "2. Token 1: \n",
      "\n",
      "3. Token 2: https://en.wikipedia.org/wiki/Reaganomics\n",
      "4. Token 3: \n",
      "\n",
      "\n",
      "5. Token 4: Reaganomics\n",
      "6. Token 5: (\n",
      "7. Token 6: a\n",
      "8. Token 7: portmanteau\n",
      "9. Token 8: of\n",
      "10. Token 9: [\n"
     ]
    }
   ],
   "source": [
    "# Question 3: Appliquer la lemmatisation avec spaCy pour les 10 premiers tokens avec index\n",
    "lemmatized_tokens_with_index = [(i, token.lemma_) for i, token in enumerate(doc[:10])]\n",
    "for index, (i, token_lemma) in enumerate(lemmatized_tokens_with_index):\n",
    "    print(f\"{index + 1}. Token {i}: {token_lemma}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mot 1: REAGANOMICS, Vecteur: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Mot 2: \n",
      ", Vecteur: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Mot 3: https://en.wikipedia.org/wiki/Reaganomics, Vecteur: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Mot 4: \n",
      "\n",
      ", Vecteur: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Mot 5: Reaganomics, Vecteur: [-1.3637e+00 -2.8468e-01  3.6431e-01  5.9614e-01  1.1314e+00 -7.4346e-01\n",
      "  1.1622e-01  1.0551e+00 -4.5270e-01  3.0553e-01  6.9261e-01 -9.6983e-01\n",
      "  7.0680e-02 -8.0307e-01 -5.7489e-01  3.9266e+00  1.1466e+00  4.7219e-01\n",
      " -2.4333e+00  3.5123e-01 -8.2377e-01  6.3726e-01  1.2146e-01 -7.9705e-02\n",
      "  1.3436e-01 -7.9861e-01 -2.0327e-01  4.4986e-01  2.1915e-01  1.6532e+00\n",
      " -9.7855e-01  7.9808e-01  1.6436e+00  2.5280e-01 -2.1583e+00 -1.0689e+00\n",
      "  7.9971e-01 -1.0594e+00 -1.7902e-01  1.8141e-02 -1.2184e+00 -3.1099e-01\n",
      " -5.4935e-01  1.0808e+00  5.4459e-01 -2.7729e-01 -6.3389e-01 -1.6596e+00\n",
      "  7.7755e-01 -1.6472e+00 -2.6032e+00  1.3839e-01  2.3209e-01  7.7527e-02\n",
      "  1.0247e+00  8.6784e-01 -2.4798e+00  2.0283e-01  1.5695e-01  1.2971e+00\n",
      "  1.3315e+00  1.0587e+00  4.5967e-01 -2.5353e-01  1.3130e+00  9.0057e-02\n",
      "  1.0769e+00 -1.9260e+00  1.7592e+00  4.3991e-01  5.7248e-01 -1.2215e-01\n",
      "  3.5562e-01  2.9468e-01  5.4795e-01  2.9964e-01 -1.7901e+00  9.9638e-02\n",
      " -1.0070e+00  4.3830e-02 -1.3926e+00  5.6919e-01  3.6508e-02  9.7977e-01\n",
      "  6.3939e-01 -2.0223e+00 -1.0357e+00 -1.6547e+00  5.6057e-01 -8.4814e-01\n",
      " -5.7233e-02  1.4735e+00  2.5684e+00 -1.9702e-01 -8.8644e-01  1.7484e+00\n",
      " -1.6157e+00  4.0931e-01  1.8060e+00  2.1953e+00  6.6551e-01 -2.7134e-01\n",
      "  1.3180e+00  1.6328e+00 -4.7331e-01  1.3364e+00  1.3869e+00 -1.2081e+00\n",
      " -7.4692e-02  1.2443e-01  8.2607e-01 -2.4295e+00 -9.0366e-01 -2.3978e-01\n",
      "  4.8814e-01  6.3107e-01  1.4194e-01 -1.1879e+00 -2.8623e+00  1.2847e+00\n",
      " -7.0615e-01 -2.8723e+00  2.3796e+00 -1.2584e+00  9.6705e-01  3.9208e-02\n",
      "  1.0173e+00 -2.2432e+00 -6.8640e-01 -2.9920e+00 -2.5758e-01 -4.2195e-01\n",
      " -1.0487e+00  7.4127e-01  1.1560e+00  1.6735e-01  4.1346e-01 -2.2555e+00\n",
      "  1.0630e+00 -3.6327e-01  2.0674e+00 -3.3638e-01  4.3100e-01  1.5483e+00\n",
      "  3.7843e-02  1.8744e+00 -2.1637e+00  1.6891e-01  5.1211e-03  2.9240e-03\n",
      "  3.6788e-01  8.0653e-01  2.6057e-01  1.3352e+00 -3.3409e-01  1.2714e-01\n",
      "  4.3431e-01  8.9266e-01 -7.5289e-01 -2.8223e-01 -4.0493e-01 -7.6418e-01\n",
      "  3.1092e-01  5.6021e-01 -1.6423e+00 -9.8192e-01 -9.4794e-01  1.1537e+00\n",
      " -4.2371e-02 -3.4928e-01  1.3747e-01 -7.5781e-01  2.0381e+00  1.0188e+00\n",
      " -8.0009e-03  1.6514e+00  9.2485e-01  3.9227e-01 -2.6485e+00 -7.7515e-01\n",
      " -4.7427e-01  9.8448e-01  1.0438e+00 -9.7849e-01  4.5655e-02 -3.7495e-01\n",
      " -5.3828e-01 -1.1668e+00  9.4683e-01  5.0669e-01  9.9413e-01  8.1948e-01\n",
      "  8.2090e-01  8.0547e-01 -2.9955e-01  2.7630e-01 -3.0304e+00 -4.0551e-01\n",
      " -9.4492e-01  1.5307e+00  7.6154e-01 -1.0264e+00  8.0278e-01  8.9488e-02\n",
      "  2.9578e+00  9.0434e-01  5.1182e-01  5.9236e-02 -7.7418e-02 -2.5760e+00\n",
      " -8.1784e-01  1.9071e-01  1.5395e+00 -2.4759e-01 -5.5463e-01 -1.2607e-01\n",
      "  9.7233e-01 -1.0847e+00  6.1186e-01  1.0796e+00  1.3763e+00  6.7768e-01\n",
      " -1.8393e+00  1.4364e+00 -9.3388e-01  2.9304e+00  1.9390e+00  3.2534e+00\n",
      " -1.0418e+00 -3.1348e-01  1.2552e-01 -2.3524e+00 -4.1014e-01  1.4575e+00\n",
      "  3.3253e-01  1.1143e+00  2.4731e+00 -5.0873e-01 -1.4758e+00  2.3212e-01\n",
      " -6.4333e-01  6.6944e-01  1.1983e+00  5.0854e-01  5.4395e-01 -4.4487e-01\n",
      " -5.6691e-01 -1.2444e-01 -2.2040e+00  1.2892e+00  2.2465e+00 -2.7195e-01\n",
      " -1.1403e+00 -2.6377e+00 -7.6080e-01  1.0348e-01 -8.0023e-01  1.5785e-01\n",
      " -3.9217e-01 -3.9416e-01 -7.8107e-02 -7.4530e-01  2.5473e+00  2.5972e+00\n",
      "  5.9505e-01  1.9050e-01  2.4204e-01  1.4186e+00 -4.3141e-01 -1.5107e+00\n",
      " -1.4291e+00 -1.1585e+00  2.7551e+00 -4.2296e-01  6.0143e-01 -1.0003e+00\n",
      " -6.8281e-01 -1.9103e-01 -3.2466e+00  8.3423e-01  1.0610e-01  8.6271e-01\n",
      " -1.5035e+00 -1.1032e+00 -7.2417e-01 -1.3407e+00  1.5049e+00 -2.7302e-01\n",
      " -8.6778e-02 -9.0648e-01 -4.3325e-02 -2.9664e+00  3.9409e-01  7.2912e-02\n",
      "  4.8735e-01  4.8375e-02  2.1387e+00  3.9454e-01  4.9088e-01 -7.6029e-01]\n",
      "Mot 6: (, Vecteur: [ -7.8392   -10.463     11.759      1.8131     0.046077  -5.9578\n",
      "   2.1858    -6.7808    11.301     -5.4783    -4.613      1.514\n",
      "   8.8983     4.1069    -1.7087     1.2146    -1.842     15.541\n",
      "   3.08      -6.7102   -10.452     -2.9291   -11.823     15.12\n",
      "  -1.7435     0.55726    1.452     -3.6608     1.4374    -3.0551\n",
      "  -9.8578    -1.2351    -1.4664     7.7278    -7.87      -1.1338\n",
      "  -0.51534   -5.0005    -6.8229    -8.1258     7.5867    -5.3086\n",
      "  -6.1498     9.2189    -4.0038     3.737     -1.4379    10.678\n",
      "  -2.968    -18.524      0.43388   -1.7291    -2.1566    11.84\n",
      "   4.545      7.1299    -9.2454    -5.6804    -5.2664    -2.2482\n",
      "  -6.8721    -3.9618    -4.0028    10.086      2.7452    -0.19343\n",
      "   8.734      2.484     -2.906     -4.1246    -0.6097    -6.9138\n",
      "  -1.2811     1.7654    -2.6818    -0.30705   -5.7846    -3.1776\n",
      "   1.3317    11.882     14.824     -6.7971   -11.288      5.8772\n",
      "  -7.272    -13.346     -5.5236     1.4068     3.2483     0.52661\n",
      "   2.682      5.4464    -0.90264    2.7181    -5.0638     2.88\n",
      " -11.875      0.78343   -2.8198     5.2037     3.3505    -1.4593\n",
      "  -3.1407     4.3899     8.2737     2.427      8.1825    -3.534\n",
      "   8.6724     3.6032    10.736     -0.68561    3.7682    -0.90342\n",
      "  -5.8552    -3.1617     8.764     11.478     -4.2121   -10.776\n",
      "   9.2309    -2.3253    10.867     -3.917     -3.6212     4.4463\n",
      "  -9.1755    -5.4338     8.6125     3.7147     1.6407     1.4826\n",
      "  -0.6922     4.6172    13.474     -3.6504     1.1869    -5.1363\n",
      " -12.805      0.59715   11.417      2.3358    -6.336     -1.9895\n",
      "   3.8036    -2.0827     5.5578    -1.0684     7.6801    -5.7769\n",
      "  -5.5995    -4.2063     2.0847     1.3778     4.4728    -0.9245\n",
      "  -6.2716     7.4471    12.234      4.7256     6.9526     9.2627\n",
      "   5.9387     3.9346    -3.9358    -5.517     -0.37221    7.0479\n",
      "  -1.0107     1.3333     0.29189    0.72582   -0.94112   -6.2164\n",
      "  -1.4666    10.999      2.8953    -2.2188   -12.6        4.9997\n",
      "  -3.3287   -10.833    -10.258     -3.1318    -2.1897    10.616\n",
      "  -2.3414    10.703    -15.958     -2.1131    -0.73608  -12.746\n",
      "   2.1645     4.7198    11.228      7.848     -9.5661    -7.8624\n",
      "   7.5167   -11.899     11.652      3.4704     8.3291     6.1333\n",
      "   1.2913    -1.6855    -4.0775    -3.8238     1.954     -0.62018\n",
      "  -4.7522    -7.3388     3.1042    -4.3236     6.6811    -3.6133\n",
      "  -4.3188    -3.6139     8.9265     6.7441     3.1195     1.3797\n",
      "  -1.7847    -1.0925     1.9636     4.6266    -1.2389    -3.6825\n",
      "   3.2352    -5.4692    -2.0027    -0.25671    5.0371    -0.47219\n",
      "  -0.56874   -0.85925   13.884      2.483      2.3522     0.62728\n",
      "   9.3776    -3.4964    -9.8182     8.8067     3.9107    -6.2802\n",
      "   0.71777   -3.2442    -6.5781    -2.0771     3.3449     0.88552\n",
      "   7.9022     6.8172     4.1641     1.4866    -1.2453    -3.4238\n",
      "  10.295     -4.0166    -9.9353    -1.2268    -4.0903    -0.70229\n",
      "  -7.2082     0.24198    5.4317    -7.3625     5.6531    -7.3531\n",
      "  -1.0492     7.3757     4.9777     9.9751    -1.9033     9.9376\n",
      "  -1.2399    -7.3147     5.5949    -0.55937   11.479     -2.8167\n",
      "   5.0842     3.1616    -7.9781    -0.39797   -4.1924     3.1309\n",
      "   3.7627     0.93319   -1.9043     2.0221    -3.6441    -4.443\n",
      "   5.5947     6.5774    -1.7344    -5.1161     8.3667    -0.72863 ]\n",
      "Mot 7: a, Vecteur: [ -9.3629     9.2761    -7.2708     4.3879    10.316     -6.8469\n",
      "   1.5755     7.9405     8.0812     2.6194    17.189      5.1028\n",
      "  -3.5406    -4.9522     0.50726    7.3238     8.4197     3.4544\n",
      "   0.83204    5.5205     5.4937     1.4897    -2.2788     4.497\n",
      "   2.3909    -9.1051    -6.827     -3.8575    -3.2794    -6.6986\n",
      "   0.14048   -2.2132     3.5909    -1.7824    -6.5155     0.23331\n",
      "   5.4186   -11.212     10.805     -9.3444    -3.3625    -1.3998\n",
      "   3.5529    -2.6246     2.5553    -1.855     -3.7859     0.29584\n",
      "  -2.5838     1.6739    -1.6049    -0.27709    1.507     -5.5291\n",
      "  -2.1429    -1.7092     8.389     -1.856     -5.4558    -6.679\n",
      "   0.36212    0.11176    1.1457    -3.2409    -9.434      1.106\n",
      "  -6.3912   -13.735      4.9788     3.9198     0.031058   4.3147\n",
      "  -6.6471     1.3955    -2.5958     6.9759    -0.99819   11.197\n",
      "  -5.3627   -13.103    -14.491     -6.1014     7.3047    -3.1905\n",
      "   1.2629    -0.92262    2.676     -0.42086   11.926     -7.8794\n",
      "  -2.3097   -10.582      5.5581    -7.7517     5.1999    -2.2761\n",
      "  -0.14107    4.6555     3.9174    -4.0377    -4.7891     2.8951\n",
      "   6.9084    12.45      -0.11449   12.134      4.8634    -2.7649\n",
      "  -9.9632    -0.24475   -3.8755    12.938     -3.9884    12.839\n",
      "  -1.8172    -2.2239    -6.8786     1.9485    10.978      3.459\n",
      "   4.5437    -6.7107    -1.8618    16.511     -2.5489   -14.822\n",
      "  -5.7842    -2.9736    10.036     -9.6984    -6.6103     2.7384\n",
      "  14.441    -13.343      1.224      3.2356    10.772     -3.7069\n",
      "   7.0112    -5.6702    -5.9481     8.3454     5.5809    12.19\n",
      "  -0.30777    7.6915     0.1935     4.1209     5.2447     8.635\n",
      "   3.2295    14.775      2.1246     7.2148     3.6015     3.3317\n",
      "  11.025      5.9624     0.33974    4.0244    -9.8245    -6.7488\n",
      "   7.9905     2.7574    -1.0628    -2.4464   -10.564     11.921\n",
      "   3.1062     0.87365   11.799     -3.936      5.7646    10.454\n",
      "   4.1789    -2.4693    -0.37961  -13.324     -8.0467    -3.0123\n",
      "  -3.4994     2.1695    -0.7963   -12.892     -6.2411     2.2934\n",
      "  -8.9541     0.1611    -3.5982     7.1816    -0.50454    3.1828\n",
      "  -5.1788    -6.4996     5.3696     1.0841    -5.2438   -13.527\n",
      "  -1.9345    -6.4937    -0.29942   -3.8215   -10.299    -12.173\n",
      "  -2.5258     1.608     -3.6306     5.5185    -8.11      -3.9384\n",
      "   7.8215     5.9596    -2.024      5.9806    12.227      2.8496\n",
      "   4.6545    -2.1338    -9.9656    -0.36781   -7.7601    -0.74959\n",
      "  -2.078      1.3549     3.7356    -9.0703     1.1395    -4.287\n",
      "  11.723     -2.7052     6.3682   -16.995      2.9882     9.7872\n",
      "  -4.2043    -3.4099    -1.0055     4.1984    -2.6362     0.56554\n",
      "   9.7921     6.6652     6.4964    11.552     -6.2677     1.7802\n",
      "  -5.8234    -1.1035    -1.3218    -0.48589    2.8878     2.1512\n",
      " -11.668     -5.8105    -8.7014     1.8706     7.656      9.1478\n",
      "   1.6743    -0.36795    4.0428     2.5721     5.1085    10.256\n",
      "  -6.9294    -0.62123   -2.4537    -0.5577    -3.5943     0.53374\n",
      "   9.6682     3.0569    -5.3577    -6.4144     3.4455     6.3063\n",
      "   5.4355     4.7154    -4.8865    -7.3881     0.37304   -5.528\n",
      "  -8.9278     3.3797    14.319     -4.0262     2.5195    -3.301\n",
      "  -9.5015     8.4454    -8.6711     3.6026     0.94914    5.9861\n",
      "   0.14368    9.7066     4.4738     2.6801    -6.816      3.5737  ]\n",
      "Mot 8: portmanteau, Vecteur: [-0.79371    0.62487    2.0744    -1.1333     0.39903    0.0047682\n",
      "  0.47186    0.18638   -0.9975     1.5512     1.4125    -0.60938\n",
      " -0.33284    1.3944     0.047433   0.050783  -0.65547    1.3548\n",
      "  1.4978    -0.46829   -0.63106    1.2086    -1.5843     1.3293\n",
      " -0.40509   -0.85122   -1.1341    -2.1388     1.0878    -0.3471\n",
      " -1.9656    -0.89838   -1.1914     0.36032    0.79617   -0.72663\n",
      " -1.2664     0.19424    0.23284   -1.8766    -0.35856   -2.2941\n",
      "  0.67216    0.64152    1.2674     0.1143    -1.8083     0.31193\n",
      "  1.2217     0.11177   -0.50601    0.58332    1.099      0.054255\n",
      "  0.8587    -1.6657     0.56116    0.79539    1.0937    -0.42369\n",
      "  0.62715   -0.15599    1.3004    -0.64949   -0.8453     1.6211\n",
      " -1.4369    -1.399      0.88118    0.47273   -1.7624    -0.22435\n",
      "  0.34247   -0.5246     0.97263   -0.74787   -0.45285   -0.12937\n",
      " -0.45591    0.51038   -2.5451     0.41271    1.5165     0.32903\n",
      "  1.3034    -1.1186     0.18644    1.8473    -1.3615    -1.7692\n",
      "  0.4211    -0.78085    2.1774    -0.45753   -0.11922   -1.4135\n",
      "  1.2768    -1.5129     0.36167   -0.73849   -1.5305    -0.31469\n",
      "  2.3808     0.96701    0.56887    0.2207    -0.16233   -0.44822\n",
      "  0.046571  -0.75599   -0.46585    0.99301    0.63845   -0.39917\n",
      " -0.43771   -0.69103    0.5122    -0.65299    0.21257   -2.8594\n",
      "  0.46636   -0.053642   1.2442     3.2774    -0.67434   -1.4637\n",
      " -2.2756    -2.2241     1.1358     0.70086   -1.1283    -0.53668\n",
      " -1.0816    -0.82708    0.68305   -0.57714    0.60058   -1.6667\n",
      "  1.9163    -1.2482    -1.0762    -0.1607     0.44226   -0.38787\n",
      "  0.18248   -1.3219    -0.36897    0.98494   -0.015323   1.1645\n",
      "  0.34349    1.4645     1.3666     0.3972    -1.61       0.16814\n",
      "  1.0615     2.3214     1.6858     0.98425    0.47674   -1.367\n",
      " -1.0161    -0.052651   0.66631   -1.9931    -0.55911    0.44264\n",
      "  0.40121    1.1159     1.0561    -0.63993   -1.3351     2.06\n",
      " -1.0222    -1.3206     1.1747    -0.91388   -0.45746    0.76937\n",
      " -1.1114    -0.11652    0.36896    0.47862   -1.3556    -0.52344\n",
      "  0.072741  -0.81312   -0.48236    0.66235    1.4202    -1.3505\n",
      "  0.5963     0.063543   0.85034    1.3399    -0.27191   -1.1163\n",
      " -0.16183    1.8824    -0.08715   -0.61832    0.70292   -0.43859\n",
      " -0.049731  -0.56194   -0.15183   -0.79504    0.15303    1.0152\n",
      "  1.2591    -0.32878   -0.7181     0.89935    1.6409     0.51523\n",
      " -0.014184  -0.46925   -1.6815    -0.42977    0.13641   -1.4845\n",
      "  0.62416    0.22052   -2.0208    -1.4297     0.78582   -0.61306\n",
      "  1.5146    -2.1401    -0.83645   -1.445     -1.1016    -0.28359\n",
      " -0.68463   -0.33809   -0.80126   -0.28233   -0.71325    1.2176\n",
      " -0.56303    0.65742    1.2792     1.2741     0.25654   -1.9683\n",
      "  1.3481    -0.56429   -0.17827    0.83004   -0.559     -2.5685\n",
      " -1.9615    -0.31341   -0.97581   -0.10572   -0.05246    0.12707\n",
      " -0.70249   -0.62644    0.16454    2.3103    -1.5639    -0.61694\n",
      " -0.11599    0.90875   -1.1093    -0.79913    0.83628   -1.9466\n",
      " -0.47049    0.36872    0.14421    0.18751   -0.93305    0.91334\n",
      "  0.95202   -0.34749   -1.0945     0.4244     1.222     -0.1628\n",
      "  1.7363     1.7384     1.6029     0.29922   -1.413      0.045947\n",
      " -0.38729    1.3108     0.86461   -0.037264   1.3929     1.2003\n",
      "  1.0617    -0.008929   0.82137    0.60192    0.44166    0.35922  ]\n",
      "Mot 9: of, Vecteur: [-12.667     -6.568     -0.61537    4.9492    22.389      2.0801\n",
      "   2.3228    11.243      9.9503    -8.4631    15.482      7.1873\n",
      "  -5.0023     0.78472   -0.5293     3.687     -0.25719    1.6776\n",
      "  -4.9576    -4.4928    -2.8919    -6.0213    -6.5132    12.358\n",
      "  14.433     -2.3726     7.4689     5.6612    -0.83442    8.3075\n",
      "   1.8904    -8.7446     2.9264   -12.088     -5.9273    -6.8889\n",
      "  -1.5864     6.1506    -4.5834     3.9353    -5.0807     1.7025\n",
      "  -6.686      2.1081    -7.6174     3.625      3.4636    10.978\n",
      "   2.6462     0.16217   -9.3867     9.8213    -0.88525   -9.6741\n",
      "   3.9125    -0.84682   -1.2667     1.8843     3.0548   -12.389\n",
      "  -2.1142    -3.0401     5.1057    -0.029813  -4.1447     4.6207\n",
      "  -3.556     -5.2786     5.2225     4.4204    -2.5684     4.42\n",
      "  -5.5672    -7.9199    -1.5292    -4.3125   -12.033     14.956\n",
      " -13.107      4.7516   -13.247     12.929     -1.7277     3.3617\n",
      "   5.4739     6.7001   -13.022    -11.402      1.5567     1.357\n",
      "  -8.5561    -2.7115     3.1369     1.2579    -5.3425   -12.055\n",
      "   3.3399    -1.3634    -6.4028     8.3323     7.5483     6.0207\n",
      "   2.711     17.107     -9.2652     6.2524    11.207     -8.1336\n",
      "  -0.63469    7.4826     5.9851     7.843    -10.373      2.8563\n",
      "   2.3344    -0.79072   10.623     13.145     -8.1474    -0.91547\n",
      "  -3.9093    -1.7907    12.998      4.3126    -7.9588   -12.959\n",
      "  -4.2593    -5.126      9.9037     1.1696     0.2393     5.3139\n",
      "   9.1737     5.1071     4.7454     8.6958    -4.7283    -7.3881\n",
      "  -5.353     -6.5763    -0.27503    2.7373     3.3596     5.2776\n",
      "   4.4331     2.6809    -8.1981    -3.8707    -2.5763     4.1216\n",
      "   2.0074     1.862     -3.4484     7.9878    -4.0771    -2.2948\n",
      "  -3.7501    -5.4446    -2.7036     3.7922     1.3469    11.879\n",
      "   6.4672     8.5983     3.8564    -0.67028    4.2223    -7.381\n",
      "   9.4206     6.688      2.8492   -11.221      9.7632    -3.3357\n",
      "  -7.2258    -4.9063     6.8963     5.2122    -7.218     -0.77411\n",
      "  -0.9108    -3.3884     5.7078    -0.5396    -0.52968  -10.289\n",
      "  -5.5984     9.9026     0.56485    0.033487  -2.4532   -13.305\n",
      "   7.4826    -8.3255     0.60487    5.0644    -1.2985     4.2398\n",
      "  -0.59503   -1.3722     1.6634    -5.8088    -7.5909    -3.5636\n",
      "  12.503      6.9943   -17.327     12.41      13.097     -1.6856\n",
      "   3.9169    14.395    -11.583     -2.9242     5.6316     4.5767\n",
      "   1.4372    -2.0338    -6.7832     5.1318    -1.8165    15.465\n",
      "  -0.87271    3.2441    -5.1716     5.3247     0.62725    5.6402\n",
      "   1.8897    -1.8938    11.417     -5.8655     3.8948    11.085\n",
      "   7.7894    18.608     -3.1164    -0.64892   -0.096788  -6.2979\n",
      "   7.0022    -0.21949   -2.4844     8.2987    -1.7999     4.5687\n",
      " -12.051     -5.9399    -3.8548     3.7942    12.426     -8.4208\n",
      " -16.285      1.4604     0.82687   -6.3887    10.292      3.2191\n",
      "  -3.3132     3.3773     3.5908    15.121     13.526      5.2244\n",
      "   2.6965     2.3931    -2.5204     2.4566     3.1298     7.3385\n",
      "  -3.8662     4.1851     9.5609    -7.1911     4.298      9.0596\n",
      "   8.894     -0.62772    4.0362     3.0782     0.82646    6.0641\n",
      "   8.9245     2.9563    -1.5334   -12.728     -1.9228     5.9504\n",
      "   7.8449    -7.1788    -5.2482     9.6856    -0.45098   -2.2526\n",
      "  -4.305      1.1597    -4.9613    -8.0021    -0.31712   -7.7062  ]\n",
      "Mot 10: [, Vecteur: [ 1.1177e+01 -7.1887e+00  9.4161e+00 -2.4650e+00 -2.8374e+00  1.4957e+00\n",
      "  6.0383e+00 -5.9013e+00 -1.3775e+01 -2.7331e+00 -3.6580e-01 -4.0579e+00\n",
      "  1.3827e+01 -8.8772e-02  3.1491e+00  9.6904e+00 -1.0800e+01  6.7037e+00\n",
      " -1.3729e+00 -6.9023e-02  7.1224e+00  5.1692e+00  3.3115e+00 -8.8179e+00\n",
      " -1.1765e+01  2.2928e+00  7.8993e+00  7.6616e+00 -5.5953e+00 -6.4187e+00\n",
      " -2.0949e+00 -3.5429e+00  9.4081e+00  4.5488e+00  1.6084e+00 -5.4542e+00\n",
      " -1.2783e+00  1.0537e+00  8.6111e+00  8.5358e+00  2.9415e+00 -3.5505e+00\n",
      " -1.1142e+01  1.1094e+01  3.9588e+00 -1.8732e+01  6.3046e+00  1.6565e+00\n",
      "  5.7985e+00 -8.9150e+00 -1.1100e+01 -1.0050e+00 -2.6484e+00 -4.9922e+00\n",
      " -6.5519e+00  6.2193e+00  8.4071e+00 -6.6907e+00  1.1004e+01 -5.0750e+00\n",
      " -1.8112e-01 -3.1464e+00 -6.4455e+00 -1.9419e+00 -5.1438e+00  5.7558e+00\n",
      " -1.2978e+01  1.2126e+00  1.8331e+00  1.4477e+00  6.9966e-01 -8.7693e+00\n",
      " -8.8283e+00 -8.4766e+00 -5.7966e-02  6.4656e-01  4.4729e+00  6.4515e+00\n",
      " -4.9925e+00  7.3460e+00 -1.2234e+00 -4.6836e+00 -4.5045e+00 -1.6942e+00\n",
      " -9.7274e+00 -1.4431e+00  3.3432e+00 -8.5028e-01  9.2050e+00  5.0326e+00\n",
      " -1.4762e+01  9.0518e-01 -1.0248e+01 -1.2740e+01 -9.2383e+00  5.5857e+00\n",
      " -3.4140e-01 -2.6039e+00 -1.6003e+00 -2.0843e+00  2.2592e+00 -3.9385e+00\n",
      "  6.3781e+00 -3.7697e+00  3.7282e+00  7.1670e+00  1.9202e+01 -2.2635e-02\n",
      " -7.0931e+00  1.0908e+01  5.1896e+00  3.0198e-01  3.6991e+00 -1.3781e+00\n",
      " -1.5276e+00  1.7613e+00 -4.0257e+00  8.2022e+00 -8.8543e-01 -2.9804e+00\n",
      "  7.1727e+00  3.1360e+00  1.7044e+00 -2.5170e+00  3.6547e+00  5.1090e+00\n",
      "  2.8281e+00  3.4796e+00 -3.2729e-01 -5.9072e+00 -1.0493e+01 -4.3558e+00\n",
      "  2.2362e+00  2.5637e-01  6.9792e+00 -9.4064e+00  8.7678e-01  2.7254e+00\n",
      " -2.8338e+00  6.8594e+00 -5.5027e+00 -5.8198e-01 -1.0075e+01  4.3286e+00\n",
      " -6.3184e+00 -6.8443e-01 -5.8576e+00  1.3814e+00  1.5747e+01  1.1588e+00\n",
      " -4.8124e+00 -1.6763e+00 -1.7892e+00  2.5894e+00 -1.5498e+00  6.6879e+00\n",
      " -4.3795e+00  5.6704e-01 -3.1004e+00  7.9931e+00  9.2267e+00  1.2749e+01\n",
      " -1.0824e+01 -6.1445e+00 -5.6653e-01 -3.1483e+00 -1.5085e+00  1.1607e+01\n",
      "  3.6702e+00  6.5091e+00 -3.6717e-03  1.0845e+01  7.1129e+00 -3.7746e+00\n",
      "  6.6342e+00 -5.3030e+00  1.0301e+01  3.0254e+00  4.1550e+00 -6.1572e+00\n",
      "  5.3851e+00 -1.8132e+01 -4.4176e+00 -8.9295e+00  3.2460e+00  5.5405e+00\n",
      "  3.1865e+00 -2.2270e+00 -1.8122e+00 -1.6677e+00  2.0687e+00  5.9035e+00\n",
      "  9.6928e+00  4.0479e+00  2.2650e+00  5.2933e+00 -9.3212e-01  1.4899e+01\n",
      " -1.3782e+00 -1.4580e+01  4.7167e+00  5.4542e-01  1.1335e+01  1.2864e+01\n",
      "  3.8424e+00  1.1484e+01 -2.2911e+00 -5.5192e+00  3.8562e+00 -7.4558e-01\n",
      " -1.2157e+01  2.2696e+00  2.4394e+00 -6.1679e-01 -4.7259e+00  1.2845e+01\n",
      " -6.5462e+00  3.0433e+00  4.9393e+00  6.9019e+00  9.1328e+00 -2.4379e+00\n",
      " -1.0256e+01 -4.5402e+00  7.5568e+00  1.7363e+01  3.9168e+00 -9.7787e+00\n",
      "  1.0059e+01  4.8659e+00 -3.4802e-01  2.9975e+00  5.8106e+00 -6.5668e+00\n",
      " -1.0778e-01 -8.8035e+00  4.4708e+00  5.7119e+00 -8.3081e+00 -1.4429e+00\n",
      "  3.4324e+00 -8.2374e+00  2.4974e+00  1.0219e-01  1.1786e+00 -9.4469e-01\n",
      "  1.1015e+01 -5.8957e+00  7.3934e+00 -1.4335e+01  1.5011e+01 -5.2170e+00\n",
      " -2.1977e+00 -2.0526e+00 -7.4385e-01 -3.1410e+00 -5.8199e-01 -5.8323e+00\n",
      " -2.8525e-01  6.7788e+00  9.0335e+00  5.3520e-01  6.8190e+00  5.5841e+00\n",
      " -4.9346e+00 -3.2754e+00 -5.0762e+00 -6.9424e-01  1.0712e+01 -8.8263e+00\n",
      "  7.5544e-01  8.1657e+00 -1.0732e-01  4.5857e+00 -2.2156e+00 -7.7073e+00\n",
      " -3.5527e+00 -1.1518e+01  7.4047e+00 -6.9977e-01  2.1871e+00  3.0568e+00\n",
      " -2.9171e+00  6.8054e+00 -1.1921e+00  4.6041e+00  6.9162e+00 -1.9474e+00\n",
      "  5.8174e+00 -2.9687e+00 -5.5419e+00  2.4223e+00  4.9028e-01 -4.7301e+00\n",
      " -1.0493e+00 -1.1068e+01 -1.0918e+00 -6.1592e+00  1.0551e+00  2.8108e+00]\n"
     ]
    }
   ],
   "source": [
    "# Question 4: Afficher les 10 premiers mots et vecteurs avec spaCy\n",
    "for i, token in enumerate(doc[:10]):\n",
    "    print(f\"Mot {i + 1}: {token.text}, Vecteur: {token.vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de sentiment: {'neg': 0.07, 'neu': 0.839, 'pos': 0.091, 'compound': 0.9982}\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = sid.polarity_scores(text)\n",
    "print(\"Score de sentiment:\", sentiment_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de similarité entre les deux premiers paragraphes: 0.7662508234165553\n"
     ]
    }
   ],
   "source": [
    "# Question 6: Analyse de similarité de document avec spaCy\n",
    "paragraphs = [sent.text for sent in doc.sents]\n",
    "similarity_score = nlp(paragraphs[0]).similarity(nlp(paragraphs[1]))\n",
    "print(\"Score de similarité entre les deux premiers paragraphes:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
